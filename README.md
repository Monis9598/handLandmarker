![68747470733a2f2f6d65646961706970652e6465762f696d616765732f6d6f62696c652f68616e645f747261636b696e675f33645f616e64726f69645f6770752e676966](https://github.com/Monis9598/handLandmarker/assets/110618321/fb4df55e-08f8-46c9-99f8-bf02a46e4a27)# handLandmarker
The ability to perceive the shape and motion of hands can be a vital component in improving the user experience
across various technological domains and platforms. For example, it can form the basis for sign language
understanding and hand gesture control. It can also enable the overlay of digital content and information on top of 
the physical world in augmented reality. 

While coming naturally to people, robust real-time hand perception is a decidedly challenging computer vision task,
as hands often occlude themselves or each other 
(e.g. finger/palm occlusions and handshakes) and lack high-contrast patterns.

MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML)
to infer 21 3D landmarks of a hand from just a single frame.
Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference, 
our method achieves real-time performance on a mobile phone 
and even scales to multiple hands.
![68747470733a2f2f6d65646961706970652e6465762f696d616765732f6d6f62696c652f68616e645f747261636b696e675f33645f616e64726f69645f6770752e676966](https://github.com/Monis9598/handLandmarker/assets/110618321/3406d278-446e-4ea3-91a0-007ff12e0f8f)

